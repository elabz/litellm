model_list:
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama1:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama2:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama3:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama4:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama5:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama6:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama7:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: L3-8B-Stheno-v3.3-32K
    litellm_params:
      model: openai/models/L3-8B-Stheno-v3.3-32K-Q5_K_M-imat.gguf
      api_key: sk-1234
      api_base: http://llama8:8080
      stream_timeout: 0.001
      rpm: 100
  - model_name: "*"
    litellm_params:
      model: openai/*
      api_key: os.environ/OPENAI_API_KEY

litellm_settings:
  # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
  drop_params: True
  # max_budget: 100 
  # budget_duration: 30d
  num_retries: 5
  request_timeout: 600
  telemetry: False
  context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]
  default_team_settings: 
    - team_id: team-1
      success_callback: ["langfuse"]
      failure_callback: ["langfuse"]
      langfuse_public_key: os.environ/LANGFUSE_PROJECT1_PUBLIC # Project 1
      langfuse_secret: os.environ/LANGFUSE_PROJECT1_SECRET # Project 1
    - team_id: team-2
      success_callback: ["langfuse"]
      failure_callback: ["langfuse"]
      langfuse_public_key: os.environ/LANGFUSE_PROJECT2_PUBLIC # Project 2
      langfuse_secret: os.environ/LANGFUSE_PROJECT2_SECRET # Project 2

router_settings:
  routing_strategy: usage-based-routing-v2 
#  redis_host: os.environ/REDIS_HOST
#  redis_password: os.environ/REDIS_PASSWORD
#  redis_port: os.environ/REDIS_PORT
  enable_pre_call_checks: true

general_settings: 
  master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
#  database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy
  database_url: "postgresql://llmproxy:dbpassword9090@db:5432/litellm" # [OPTIONAL] use for token-based auth to proxy

# environment_variables:
  # settings for using redis caching
#  REDIS_HOST: redis
#  REDIS_PORT: "6379"
#  REDIS_PASSWORD: "My*secreT"
